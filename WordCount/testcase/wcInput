cloud computing from Wikipediac the free encyclopedia cloud Computing logical diagram cloud computing is the Use of computing resources chardware and softwarec that are delivered as a service over a network ctypically the internetcc the name comes from the use of a cloudcshaped symbol as an abstraction for the complex infrastructure it contains in system diagramsc cloud computing entrusts remote services with a usercs datac software and computationc there are many types of public cloud computingcccc infrastructure as a service ciaasc platform as a service cpaasc software as a service csaasc storage as a service cstaasc security as a service csecaasc data as a service cdaasc test environment as a service cteaasc desktop as a service cdaasc api as a service capiaasc the business modelc it as a service citaascc is used by inchousec enterprise it organizations that offer any or all of the above servicesc using software as a servicec users also rent application software and databasesc the cloud providers manage the infrastructure and platforms on which the applications runc end users access cloudcbased applications through a web browser or a lightcweight desktop or mobile app while the business software and usercs data are stored on servers at a remote locationc proponents claim that cloud computing allows enterprises to get their applications up and running fasterc with improved manageability and less maintenancec and enables it to more rapidly adjust resources to meet fluctuating and unpredictable business demandccccccc cloud computing relies on sharing of resources to achieve coherence and economies of scale similar to a utility clike the electricity gridc over a networkcccc at the foundation of cloud computing is the broader concept of converged infrastructure and shared servicesc contents chidec c history c similar systems and concepts c characteristics ccc oncdemand selfcservice c service models ccc infrastructure as a service ciaasc ccc platform as a service cpaasc ccc software as a service csaasc c cloud clients c deployment models ccc public cloud ccc community cloud ccc hybrid cloud ccc private cloud c architecture ccc the intercloud ccc cloud engineering c issues ccc privacy ccc compliance ccc legal ccc open source ccc open standards ccc security ccc sustainability ccc abuse ccc it governance c research cc see also cc references cc external links ceditchistory this section requires expansionc cjune ccccc the origin of the term cloud computing is obscurec but it appears to derive from the practice of using drawings of stylized clouds to denote networks in diagrams of computing and communications systemsc the word cloud is used as a metaphor for the internetc based on the standardized use of a cloudclike shape to denote a network on telephony schematics and later to depict the internet in computer network diagrams as an abstraction of the underlying infrastructure it representsc the cloud symbol was used to represent the internet as early as ccccccccccc in the ccccsc telecommunications companies who previously offered primarily dedicated pointctocpoint data circuitsc began offering virtual private network cvpnc services with comparable quality of service but at a much lower costc by switching traffic to balance utilization as they saw fitc they were able to utilize their overall network bandwidth more effectivelyc the cloud symbol was used to denote the demarcation point between that which was the responsibility of the provider and that which was the responsibility of the usersc cloud computing extends this boundary to cover servers as well as the network infrastructurecccc the underlying concept of cloud computing dates back to the ccccsc when largecscale mainframe became available in academia and corporationsc accessible via thin clients c terminal computersc because it was costly to buy a mainframec it became important to find ways to get the greatest return on the investment in themc allowing multiple users to share both the physical access to the computer from multiple terminals as well as to share the cpu timec eliminating periods of inactivityc which became known in the industry as timecsharingcccc as computers became more prevalentc scientists and technologists explored ways to make largecscale computing power available to more users through time sharingc experimenting with algorithms to provide the optimal use of the infrastructurec platform and applications with prioritized access to the cpu and efficiency for the end userscccc john mccarthy opined in the ccccs that ccomputation may someday be organized as a public utilitycc almost all the moderncday characteristics of cloud computing celastic provisionc provided as a utilityc onlinec illusion of infinite supplycc the comparison to the electricity industry and the use of publicc privatec governmentc and community formsc were thoroughly explored in douglas parkhillcs cccc bookc the challenge of the computer utilityc other scholars have shown that cloud computingcs roots go all the way back to the ccccs when scientist herb grosch cthe author of groschcs lawc postulated that the entire world would operate on dumb terminals powered by about cc large data centersccccc due to the expense of these powerful computersc many corporations and other entities could avail themselves of computing capability through time sharing and several organizationsc such as gecs geiscoc ibm subsidiary the service bureau corporation csbcc founded in cccccc tymshare cfounded in cccccc national css cfounded in cccc and bought by dun c bradstreet in cccccc dial data cbought by tymshare in cccccc and boltc beranek and newman cbbnc marketed time sharing as a commercial venturec the development of the internet from being document centric via semantic data towards more and more services was described as cdynamic webcccccc this contribution focused in particular in the need for better metacdata able to describe not only implementation details but also conceptual details of modelcbased applicationsc the ubiquitous availability of highccapacity networksc lowccost computers and storage devices as well as the widespread adoption of hardware virtualizationc servicecoriented architecturec autonomicc and utility computing have led to a tremendous growth in cloud computingccccccccccccc after the dotccom bubblec amazon played a key role in the development of cloud computing by modernizing their data centersc whichc like most computer networksc were using as little as ccc of their capacity at any one timec just to leave room for occasional spikesc having found that the new cloud architecture resulted in significant internal efficiency improvements whereby smallc fastcmoving ctwocpizza teamsc cteams small enough to be fed with two pizzasc could add new features faster and more easilyc amazon initiated a new product development effort to provide cloud computing to external customersc and launched amazon web service cawsc on a utility computing basis in ccccccccccccc in late cccc ben dorsictodaro demonstrated in his tech me out blog post titled we are obsolete a crude yet affordable method for an average computer user to achieve a the basic concept of cloud computing from the comfort of a personcs own homec in early ccccc eucalyptus became the first opencsourcec aws apiccompatible platform for deploying private cloudsc in early ccccc opennebulac enhanced in the reservoir european commissioncfunded projectc became the first opencsource software for deploying private and hybrid cloudsc and for the federation of cloudsccccc in the same yearc efforts were focused on providing quality of service guarantees cas required by realctime interactive applicationsc to cloudcbased infrastructuresc in the framework of the irmos european commissioncfunded projectc resulting to a realctime cloud environmentccccc by midcccccc gartner saw an opportunity for cloud computing cto shape the relationship among consumers of it servicesc those who use it services and those who sell themccccc and observed that corganisations are switching from companycowned hardware and software assets to percuse servicecbased modelsc so that the cprojected shift to computingccc will result in dramatic growth in it products in some areas and significant reductions in other areascccccc on march cc ccccc ibm announced the smarter computing framework to support smarter planetccccc among the various components of the smarter computing foundationc cloud computing is a critical piecec in ccccc drc biju john and drc souheil khaddaj describe the cloud as a virtualizedc semantic source of informationc ccloud computing is a universal collection of data which extends over the internet in the form of resources csuch as information hardwarec various platformsc services etccc and forms individual units within the virtualization environmentc held together by infrastructure providersc service providers and the consumerc then it is semantically accessed by various usersccccitation neededc ceditcsimilar systems and concepts cloud computing shares characteristics withc autonomic computing cx computer systems capable of selfcmanagementccccc clientcvserver model cx clientcvserver computing refers broadly to any distributed application that distinguishes between service providers cserversc and service requesters cclientscccccc grid computing cx ca form of distributed and parallel computingc whereby a csuper and virtual computerc is composed of a cluster of networkedc loosely coupled computers acting in concert to perform very large taskscc mainframe computer cx powerful computers used mainly by large organizations for critical applicationsc typically bulk data processing such as censusc industry and consumer statisticsc police and secret intelligence servicesc enterprise resource planningc and financial transaction processingccccc utility computing cx the cpackaging of computing resourcesc such as computation and storagec as a metered service similar to a traditional public utilityc such as electricitycccccccccc peerctocpeer cx distributed architecture without the need for central coordinationc with participants being at the same time both suppliers and consumers of resources cin contrast to the traditional clientcvserver modelcc cloud gaming c also called oncdemand gaming is a way of delivering to games to computersc the gaming data will be stored in the providercs serverc so that gaming will be independent of client computers used to play the gamec ceditccharacteristics cloud computing exhibits the following key characteristicsc agility improves with usersc ability to recprovision technological infrastructure resourcesc application programming interface capic accessibility to software that enables machines to interact with cloud software in the same way the user interface facilitates interaction between humans and computersc cloud computing systems typically use restcbased apisc cost is claimed to be reduced and in a public cloud delivery model capital expenditure is converted to operational expenditureccccc this is purported to lower barriers to entryc as infrastructure is typically provided by a thirdcparty and does not need to be purchased for onectime or infrequent intensive computing tasksc pricing on a utility computing basis is finecgrained with usagecbased options and fewer it skills are required for implementation cinchousecccccc the ecfiscal projectcs state of the art repositorycccc contains several articles looking into cost aspects in more detailc most of them concluding that costs savings depend on the type of activities supported and the type of infrastructure available inchousec device and location independencecccc enable users to access systems using a web browser regardless of their location or what device they are using cecgcc pcc mobile phonecc as infrastructure is offcsite ctypically provided by a thirdcpartyc and accessed via the internetc users can connect from anywhereccccc virtualization technology allows servers and storage devices to be shared and utilization be increasedc applications can be easily migrated from one physical server to anotherc multitenancy enables sharing of resources and costs across a large pool of users thus allowing forc centralization of infrastructure in locations with lower costs csuch as real estatec electricityc etccc peakcload capacity increases cusers need not engineer for highest possible loadclevelsc utilisation and efficiency improvements for systems that are often only cccvccc utilisedccccc reliability is improved if multiple redundant sites are usedc which makes wellcdesigned cloud computing suitable for business continuity and disaster recoveryccccc scalability and elasticity via dynamic cconcdemandcc provisioning of resources on a finecgrainedc selfcservice basis near realctimec without users having to engineer for peak loadsccccccccc performance is monitoredc and consistent and loosely coupled architectures are constructed using web services as the system interfaceccccc security could improve due to centralization of datac increased securitycfocused resourcesc etccc but concerns can persist about loss of control over certain sensitive datac and the lack of security for stored kernelsccccc security is often as good as or better than other traditional systemsc in part because providers are able to devote resources to solving security issues that many customers cannot affordccccc howeverc the complexity of security is greatly increased when data is distributed over a wider area or greater number of devices and in multictenant systems that are being shared by unrelated usersc in additionc user access to security audit logs may be difficult or impossiblec private cloud installations are in part motivated by usersc desire to retain control over the infrastructure and avoid losing control of information securityc maintenance of cloud computing applications is easierc because they do not need to be installed on each usercs computer and can be accessed from different placesc ceditconcdemand selfcservice see alsoc selfcservice provisioning for cloud computing services and service catalogs for cloud computing services oncdemand selfcservice allows users to obtainc configure and deploy cloud services themselves using cloud service cataloguesc without requiring the assistance of itccccccccc this feature is listed by the the national institute of standards and technology cnistc as a characteristic of cloud computingccccc the selfcservice requirement of cloud computing prompts infrastructure vendors to create cloud computing templatesc which are obtained from cloud service cataloguesc manufacturers of such templates or blueprints include hewlettcpackard chpcc which names its templates as hp cloud mapscccc rightscalecccc and red hatc which names its templates cloudformsccccc the templates contain predefined configurations used to by consumers to set up cloud servicesc the templates or blueprints provide the technical information necessary to build readyctocuse cloudsccccc each template includes specific configuration details for different cloud infrastructuresc with information about servers for specific tasks such as hosting applicationsc databasesc websites and so onccccc the templates also include predefined web servicec the operating systemc the databasec security configurations and load balancingccccc cloud consumers use cloud templates to move applications between clouds through a selfcservice portalc the predefined blueprints define all that an application requires to run in different environmentsc for examplec a template could define how the same application could be deployed in cloud platforms based on amazon web servicec vmware or red hatccccc the user organisation benefits from cloud templates because the technical aspects of cloud configurations reside in the templatesc letting users to deploy cloud services with a push of a buttonccccccccc cloud templates can also be used by developers to create a catalog of cloud servicesccccc ceditcservice models cloud computing providers offer their services according to three fundamental modelscccccccc infrastructure as a service ciaascc platform as a service cpaascc and software as a service csaasc where iaas is the most basic and each higher model abstracts from the details of the lower modelsc ceditcinfrastructure as a service ciaasc see alsoc categoryccloud infrastructure in this most basic cloud service modelc cloud providers offer computersc as physical or more often as virtual machinesc and other resourcesc the virtual machines are run as guests by a hypervisorc such as xen or kvmc management of pools of hypervisors by the cloud operational support system leads to the ability to scale to support a large number of virtual machinesc other resources in iaas clouds include images in a virtual machine image libraryc raw cblockc and filecbased storagec firewallsc load balancersc ip addressesc virtual local area networks cvlanscc and software bundlesccccc amiesc alexc sluimanc harmc tong iaas cloud providers supply these resources on demand from their large pools installed in data centersc for wide area connectivityc the internet can be used orcxin carrier clouds cc dedicated virtual private networks can be configuredcc qiang guo cjuly cccccc cinfrastructure as a service cloud conceptscc developing and hosting applications on the cloudc ibm pressc isbn cccccccccccccccccc to deploy their applicationsc cloud users then install operating system images on the machines as well as their application softwarec in this modelc it is the cloud user who is responsible for patching and maintaining the operating systems and application softwarec cloud providers typically bill iaas services on a utility computing basisc that isc cost will reflect the amount of resources allocated and consumedc iaas refers not to a machine that does all the workc but simply to a facility given to businesses that offers users the leverage of extra storage space in servers and data centersc examples of iaas includec amazon cloudformation cand underlying services such as amazon ecccc rackspace cloudc terremark and google compute enginec ceditcplatform as a service cpaasc main articlec platform as a service see alsoc categoryccloud platforms in the paas modelc cloud providers deliver a computing platform typically including operating systemc programming language execution environmentc databasec and web serverc application developers can develop and run their software solutions on a cloud platform without the cost and complexity of buying and managing the underlying hardware and software layersc with some paas offersc the underlying computer and storage resources scale automatically to match application demand such that cloud user does not have to allocate resources manuallyc examples of paas includec amazon elastic beanstalkc cloud foundryc herokuc forceccomc engineyardc mendixc google app enginec microsoft azure and orangescapec ceditcsoftware as a service csaasc main articlec software as a service in this modelc cloud providers install and operate application software in the cloud and cloud users access the software from cloud clientsc the cloud users do not manage the cloud infrastructure and platform on which the application is runningc this eliminates the need to install and run the application on the cloud usercs own computers simplifying maintenance and supportc what makes a cloud application different from other applications is its elasticityc this can be achieved by cloning tasks onto multiple virtual machines at runctime to meet the changing work demandccccc load balancers distribute the work over the set of virtual machinesc this process is inconspicuous to the cloud user who sees only a single access pointc to accommodate a large number of cloud usersc cloud applications can be multitenantc that isc any machine serves more than one cloud user organizationc it is common to refer to special types of cloud based application software with a similar naming conventionc desktop as a servicec business process as a servicec test environment as a servicec communication as a servicec the pricing model for saas applications is typically a monthly or yearly flat fee per userccccc so price is scalable and adjustable if users are added or removed at any pointccccc examples of saas includec google appsc innkeyposc quickbooks onlinec limelight video platformc salesforceccom and microsoft office cccc ceditccloud clients see alsoc categoryccloud clients users access cloud computing using networked client devicesc such as desktop computersc laptopsc tablets and smartphonesc some of these devices c cloud clients c rely on cloud computing for all or a majority of their applications so as to be essentially useless without itc examples are thin clients and the browsercbased chromebookc many cloud applications do not require specific software on the client and instead use a web browser to interact with the cloud applicationc with ajax and htmlc these web user interfaces can achieve a similar or even better look and feel as native applicationsc some cloud applicationsc howeverc support specific client software dedicated to these applications cecgcc virtual desktop clients and most email clientscc some legacy applications cline of business applications that until now have been prevalent in thin client windows computingc are delivered via a screencsharing technologyc ceditcdeployment models cloud computing types ceditcpublic cloud public cloud applicationsc storagec and other resources are made available to the general public by a service providerc these services are free or offered on a paycpercuse modelc generallyc public cloud service providers like amazon awsc microsoft and google own and operate the infrastructure and offer access only via internet cdirect connectivity is not offeredcccccc ceditccommunity cloud community cloud shares infrastructure between several organizations from a specific community with common concerns csecurityc compliancec jurisdictionc etcccc whether managed internally or by a thirdcparty and hosted internally or externallyc the costs are spread over fewer users than a public cloud cbut more than a private cloudcc so only some of the cost savings potential of cloud computing are realizedcccc ceditchybrid cloud hybrid cloud is a composition of two or more clouds cprivatec community or publicc that remain unique entities but are bound togetherc offering the benefits of multiple deployment modelscccc by utilizing chybrid cloudc architecturec companies and individuals are able to obtain degrees of fault tolerance combined with locally immediate usability without dependency on internet connectivityc hybrid cloud architecture requires both oncpremises resources and offcsite cremotec servercbased cloud infrastructurec hybrid clouds lack the flexibilityc security and certainty of inchouse applicationsccccc hybrid cloud provides the flexibility of in house applications with the fault tolerance and scalability of cloud based servicesc ceditcprivate cloud private cloud is cloud infrastructure operated solely for a single organizationc whether managed internally or by a thirdcparty and hosted internally or externallycccc undertaking a private cloud project requires a significant level and degree of engagement to virtualize the business environmentc and it will require the organization to reevaluate decisions about existing resourcesc when it is done rightc it can have a positive impact on a businessc but every one of the steps in the project raises security issues that must be addressed in order to avoid serious vulnerabilitiesccccc they have attracted criticism because users cstill have to buyc buildc and manage themc and thus do not benefit from less handscon managementccccc essentially cclackingc the economic model that makes cloud computing such an intriguing conceptcccccccccc ceditcarchitecture cloud computing sample architecture cloud architectureccccc the systems architecture of the software systems involved in the delivery of cloud computingc typically involves multiple cloud components communicating with each other over a loose coupling mechanism such as a messaging queuec elastic provision implies intelligence in the use of tight or loose coupling as applied to mechanisms such as these and othersc ceditcthe intercloud main articlec intercloud the intercloudcccc is an interconnected global ccloud of cloudsccccccccc and an extension of the internet cnetwork of networksc on which it is basedccccccccccccc ceditccloud engineering cloud engineering is the application of engineering disciplines to cloud computingc it brings a systematic approach to the highclevel concerns of commercialisationc standardisationc and governance in conceivingc developingc operating and maintaining cloud computing systemsc it is a multidisciplinary method encompassing contributions from diverse areas such as systemsc softwarec webc performancec informationc securityc platformc riskc and quality engineeringc ceditcissues ceditcprivacy the cloud model has been criticised by privacy advocates for the greater ease in which the companies hosting the cloud services controlc thusc can monitor at willc lawfully or unlawfullyc the communication and data stored between the user and the host companyc instances such as the secret nsa programc working with atctc and verizonc which recorded over cc million phone calls between american citizensc causes uncertainty among privacy advocatesc and the greater powers it gives to telecommunication companies to monitor user activityccccc using a cloud service provider ccspc can complicate privacy of data because of the extent to which virtualization for cloud processing cvirtual machinesc and cloud storage are used to implement cloud serviceccccc the point is that csp operationsc customer or tenant data may not remain on the same systemc or in the same data center or even within the same providercs cloudc this can lead to legal concerns over jurisdictionc while there have been efforts csuch as usceu safe harborc to charmonisec the legal environmentc providers such as amazon still cater to major markets ctypically the united states and the european unionc by deploying local infrastructure and allowing customers to select cavailability zonescccccc cloud computing poses privacy concerns because the service provider may access the data that is on the cloud at any point in timec they could accidentally or deliberately alter or even delete informationccccc postage and delivery services companyc pitney bowes launched vollyc a cloudcbasedc digital mailbox service to leverage its communication management assetsc they also faced the technical challenge of providing strong data security and privacyc howeverc they were able to address the same concern by applying customizedc applicationclevel securityc including encryptionc cccc ceditccompliance in order to obtain compliance with regulations including fismac hipaac and sox in the united statesc the data protection directive in the eu and the credit card industrycs pci dssc users may have to adopt community or hybrid deployment modes that are typically more expensive and may offer restricted benefitsc this is how google is able to cmanage and meet additional government policy requirements beyond fismaccccccccc and rackspace cloud or qubespace are able to claim pci complianceccccc many providers also obtain a sas cc type ii auditc but this has been criticised on the grounds that the handcpicked set of goals and standards determined by the auditor and the auditee are often not disclosed and can vary widelyccccc providers typically make this information available on requestc under noncdisclosure agreementccccccccc customers in the eu contracting with cloud providers outside the euceea have to adhere to the eu regulations on export of personal dataccccc ucsc federal agencies have been directed by the office of management and budget to use a process called fedramp cfederal risk and authorization management programc to assess and authorize cloud products and servicesc federal cio steven vanroekel issued a memorandum to federal agency chief information officers on december cc cccc defining how federal agencies should use fedrampc fedramp consists of a subset of nist special publication cccccc security controls specifically selected to provide protection in cloud environmentsc a subset has been defined for the fips ccc low categorization and the fips ccc moderate categorizationc the fedramp program has also established a joint accreditation board cjabc consisting of chief information officers from dodc dhs and gsac the jab is responsible for establishing accreditation standards for crd party organizations who will perform the assessments of cloud solutionsc the jab will also review authorization packages and may grant provisional authorization cto operatecc the federal agency consuming the service will still have the final responsibility for final authority to operateccccc ceditclegal this section requires expansion withc examples and additional citationsc caugust ccccc as can be expected with any revolutionary change in the landscape of global computingc certain legal issues arisec everything from trademark infringementc security concerns to the sharing of propriety data resourcesc ceditcopen source see alsoc categorycfree software for cloud computing opencsource software has provided the foundation for many cloud computing implementationsc prominent examples being the hadoop frameworkcccc and vmwarecs cloud foundryccccc in november ccccc the free software foundation released the affero general public licensec a version of gplvc intended to close a perceived legal loophole associated with free software designed to be run over a networkccccc ceditcopen standards see alsoc categoryccloud standards most cloud providers expose apis that are typically wellcdocumented coften under a creative commons licenseccccc but also unique to their implementation and thus not interoperablec some vendors have adopted othersc apis and there are a number of open standards under developmentc with a view to delivering interoperability and portabilityccccc ceditcsecurity main articlec cloud computing security as cloud computing is achieving increased popularityc concerns are being voiced about the security issues introduced through adoption of this new modelc the effectiveness and efficiency of traditional protection mechanisms are being reconsidered as the characteristics of this innovative deployment model can differ widely from those of traditional architecturesccccc an alternative perspective on the topic of cloud security is that this is but anotherc although quite broadc case of capplied securityc and that similar security principles that apply in shared multicuser mainframe security models apply with cloud securityccccc the relative security of cloud computing services is a contentious issue that may be delaying its adoptionccccc physical control of the private cloud equipment is more secure than having the equipment off site and under someone elseccs controlc physical control and the ability to visually inspect the data links and access ports is required in order to ensure data links are not compromisedc issues barring the adoption of cloud computing are due in large part to the private and public sectorsc unease surrounding the external management of securitycbased servicesc it is the very nature of cloud computingcbased servicesc private or publicc that promote external management of provided servicesc this delivers great incentive to cloud computing service providers to prioritize building and maintaining strong management of secure servicesccccc security issues have been categorised into sensitive data accessc data segregationc privacyc bug exploitationc recoveryc accountabilityc malicious insidersc management console securityc account controlc and multictenancy issuesc solutions to various cloud security issues varyc from cryptographyc particularly public key infrastructure cpkicc to use of multiple cloud providersc standardisation of apisc and improving virtual machine support and legal supportccccccccccccc cloud computing offers many benefitsc but it also is vulnerable to threatsc as the uses of cloud computing increasec it is highly likely that more criminals will try to find new ways to exploit vulnerabilities in the systemc there are many underlying challenges and risks in cloud computing that increase the threat of data being compromisedc to help mitigate the threatc cloud computing stakeholders should invest heavily in risk assessment to ensure that the system encrypts to protect datac establishes trusted foundation to secure the platform and infrastructurec and builds higher assurance into auditing to strengthen compliancec security concerns must be addressed in order to establish trust in cloud computing technologyc ceditcsustainability although cloud computing is often assumed to be a form of cgreen computingcc there is no published study to substantiate this assumptionccccc citing the servers affects the environmental effects of cloud computingc in areas where climate favors natural cooling and renewable electricity is readily availablec the environmental effects will be more moderatec cthe same holds true for ctraditionalc data centerscc thus countries with favorable conditionsc such as finlandccccc sweden and switzerlandccccc are trying to attract cloud computing data centersc energy efficiency in cloud computing can result from energycaware scheduling and server consolidationccccc howeverc in the case of distributed clouds over data centers with different source of energies including renewable source of energiesc a small compromise on energy consumption reduction could result in high carbon footprint reductionccccc ceditcabuse as with privately purchased hardwarec customers can purchase the services of cloud computing for nefarious purposesc this includes password cracking and launching attacks using the purchased servicesccccc in ccccc a banking trojan illegally used the popular amazon service as a command and control channel that issued software updates and malicious instructions to pcs that were infected by the malwareccccc ceditcit governance main articlec corporate governance of information technology the introduction of cloud computing requires an appropriate it governance model to ensure a secured computing environment and to comply with all relevant organizational information technology policiesccccccccc as suchc organizations need a set of capabilities that are essential when effectively implementing and managing cloud servicesc including demand managementc relationship managementc data security managementc application lifecycle managementc risk and compliance managementccccc ceditcresearch many universitiesc vendors and government organisations are investing in research around the topic of cloud computingccccccccc in october ccccc the academic cloud computing initiative caccic was announced as a multicuniversity project designed to enhance studentsc technical knowledge to address the challenges of cloud computingccccc in april ccccc uc santa barbara released the first open source platformcascacservicec appscalec which is capable of running google app engine applications at scale on a multitude of infrastructuresc in april ccccc the st andrews cloud computing coclaboratory was launchedc focusing on research in the important new area of cloud computingc unique in the ukc stacc aims to become an international centre of excellence for research and teaching in cloud computing and will provide advice and information to businesses interested in using cloudcbased servicescccccc in october ccccc the tclouds ctrustworthy cloudsc project was startedc funded by the european commissioncs cth framework programmec the projectcs goal is to research and inspect the legal foundation and architectural design to build a resilient and trustworthy cloudcofccloud infrastructure on top of thatc the project also develops a prototype to demonstrate its resultscccccc in december ccccc the trustcloud research project cccccccccc was started by hp labs singapore to address transparency and accountability of cloud computing via detectivec dataccentric approachesccccc encapsulated in a fiveclayer trustcloud frameworkc the team identified the need for monitoring data life cycles and transfers in the cloudcccccc leading to the tackling of key cloud computing security issues such as cloud data leakagesc cloud accountability and crosscnational data transfers in transnational cloudsc in july ccccc the high performance computing cloud chpccloudc project was kickedcoff aiming at finding out the possibilities of enhancing performance on cloud environments while running the scientific applications c development of hpccloud performance analysis toolkit which was funded by cimcreturning experts programme c under the coordination of profc drc shajulin benedictc in june ccccc the telecommunications industry association developed a cloud computing white paperc to analyze the integration challenges and opportunities between cloud services and traditional ucsc telecommunications standardscccccc in ccccc femhub launched nclabc a free saas application for sciencec technologyc engineering and mathematics cstemcc nclab has more than cccccc users as of july ccccc ceditcsee also mapreduce from wikipediac the free encyclopedia mapreduce is a programming model for processing large data setsc and the name of an implementation of the model by googlec mapreduce is typically used to do distributed computing on clusters of computerscccc the model is inspired by the map and reduce functions commonly used in functional programmingcccc although their purpose in the mapreduce framework is not the same as their original formscccc mapreduce libraries have been written in many programming languagesc a popular free implementation is apache hadoopc contents chidec c overview c logical view ccc example c dataflow ccc input reader ccc map function ccc partition function ccc comparison function ccc reduce function ccc output writer c distribution and reliability c uses c criticism c conferences and users groups c see also c references cc external links ceditcoverview mapreduce is a framework for processing embarrassingly parallel problems across huge datasets using a large number of computers cnodescc collectively referred to as a cluster cif all nodes are on the same local network and use similar hardwarec or a grid cif the nodes are shared across geographically and administratively distributed systemsc and use more heterogenous hardwarecc computational processing can occur on data stored either in a filesystem cunstructuredc or in a database cstructuredcc mapreduce can take advantage of locality of datac processing data on or near the storage assets to decrease transmission of datac cmapc stepc the master node takes the inputc divides it into smaller subcproblemsc and distributes them to worker nodesc a worker node may do this again in turnc leading to a multiclevel tree structurec the worker node processes the smaller problemc and passes the answer back to its master nodec creducec stepc the master node then collects the answers to all the subcproblems and combines them in some way to form the output cv the answer to the problem it was originally trying to solvec mapreduce allows for distributed processing of the map and reduction operationsc provided each mapping operation is independent of the othersc all maps can be performed in parallel cv though in practice it is limited by the number of independent data sources andcor the number of cpus near each sourcec similarlyc a set of creducersc can perform the reduction phase c provided all outputs of the map operation that share the same key are presented to the same reducer at the same timec or if the reduction function is associativec while this process can often appear inefficient compared to algorithms that are more sequentialc mapreduce can be applied to significantly larger datasets than ccommodityc servers can handle cv a large server farm can use mapreduce to sort a petabyte of data in only a few hoursc the parallelism also offers some possibility of recovering from partial failure of servers or storage during the operationc if one mapper or reducer failsc the work can be rescheduled cv assuming the input data is still availablec ceditclogical view the map and reduce functions of mapreduce are both defined with respect to data structured in ckeyc valuec pairsc map takes one pair of data with a type in one data domainc and returns a list of pairs in a different domainc mapckccvcc cc listckccvcc the map function is applied in parallel to every pair in the input datasetc this produces a list of pairs for each callc after thatc the mapreduce framework collects all pairs with the same key from all lists and groups them togetherc thus creating one group for each one of the different generated keysc the reduce function is then applied in parallel to each groupc which in turn produces a collection of values in the same domainc reduceckcc list cvccc cc listcvcc each reduce call typically produces either one value vc or an empty returnc though one call is allowed to return more than one valuec the returns of all calls are collected as the desired result listc thus the mapreduce framework transforms a list of ckeyc valuec pairs into a list of valuesc this behavior is different from the typical functional programming map and reduce combinationc which accepts a list of arbitrary values and returns one single value that combines all the values returned by mapc it is necessary but not sufficient to have implementations of the map and reduce abstractions in order to implement mapreducec distributed implementations of mapreduce require a means of connecting the processes performing the map and reduce phasesc this may be a distributed file systemc other options are possiblec such as direct streaming from mappers to reducersc or for the mapping processors to serve up their results to reducers that query themc ceditcexample the canonical example application of mapreduce is a process to count the appearances of each different word in a set of documentsc function mapcstring namec string documentcc cc namec document name cc documentc document contents for each word w in documentc emit cwc cc function reducecstring wordc iterator partialcountscc cc wordc a word cc partialcountsc a list of aggregated partial counts sum c c for each pc in partialcountsc sum cc pc emit cwordc sumc herec each document is split into wordsc and each word is counted by the map functionc using the word as the result keyc the framework puts together all the pairs with the same key and feeds them to the same call to reducec thus this function just needs to sum all of its input values to find the total appearances of that wordc ceditcdataflow the frozen part of the mapreduce framework is a large distributed sortc the hot spotsc which the application definesc arec an input reader a map function a partition function a compare function a reduce function an output writer ceditcinput reader the input reader divides the input into appropriate size csplitsc cin practice typically cc mb to ccc mbc and the framework assigns one split to each map functionc the input reader reads data from stable storage ctypically a distributed file systemc and generates keycvalue pairsc a common example will read a directory full of text files and return each line as a recordc ceditcmap function each map function takes a series of keycvalue pairsc processes eachc and generates zero or more output keycvalue pairsc the input and output types of the map can be cand often arec different from each otherc if the application is doing a word countc the map function would break the line into words and output a keycvalue pair for each wordc each output pair would contain the word as the key and ccc as the valuec ceditcpartition function each map function output is allocated to a particular reducer by the applicationcs partition function for sharding purposesc the partition function is given the key and the number of reducers and returns the index of the desired reducec a typical default is to hash the key and modulo the number of reducersc it is important to pick a partition function that gives an approximately uniform distribution of data per shard for load balancing purposesc otherwise the mapreduce operation can be held up waiting for slow reducers to finishc between the map and reduce stagesc the data is shuffled cparallelcsorted c exchanged between nodesc in order to move the data from the map node that produced it to the shard in which it will be reducedc the shuffle can sometimes take longer than the computation time depending on network bandwidthc cpu speedsc data produced and time taken by map and reduce computationsc ceditccomparison function the input for each reduce is pulled from the machine where the map ran and sorted using the applicationcs comparison functionc ceditcreduce function the framework calls the applicationcs reduce function once for each unique key in the sorted orderc the reduce can iterate through the values that are associated with that key and produce zero or more outputsc in the word count examplec the reduce function takes the input valuesc sums them and generates a single output of the word and the final sumc ceditcoutput writer the output writer writes the output of the reduce to stable storagec usually a distributed file systemc ceditcdistribution and reliability mapreduce achieves reliability by parceling out a number of operations on the set of data to each node in the networkc each node is expected to report back periodically with completed work and status updatesc if a node falls silent for longer than that intervalc the master node csimilar to the master server in the google file systemc records the node as dead and sends out the nodecs assigned work to other nodesc individual operations use atomic operations for naming file outputs as a check to ensure that there are not parallel conflicting threads runningc when files are renamedc it is possible to also copy them to another name in addition to the name of the task callowing for sideceffectscc the reduce operations operate much the same wayc because of their inferior properties with regard to parallel operationsc the master node attempts to schedule reduce operations on the same nodec or in the same rack as the node holding the data being operated onc this property is desirable as it conserves bandwidth across the backbone network of the datacenterc implementations are not necessarily highlycreliablec for examplec in hadoop the namenode is a single point of failure for the distributed filesystemc ceditcuses mapreduce is useful in a wide range of applicationsc including distributed patterncbased searchingc distributed sortc web linkcgraph reversalc termcvector per hostc web access log statsc inverted index constructionc document clusteringc machine learningcccc and statistical machine translationc moreoverc the mapreduce model has been adapted to several computing environments like multiccore and manyccore systemsccccccc desktop gridscccc volunteer computing environmentscccc dynamic cloud environmentscccc and mobile environmentsccccc at googlec mapreduce was used to completely regenerate googlecs index of the world wide webc it replaced the old ad hoc programs that updated the index and ran the various analysesccccc mapreducecs stable inputs and outputs are usually stored in a distributed file systemc the transient data is usually stored on local disk and fetched remotely by the reducersc ceditccriticism david dewitt and michael stonebrakerc computer scientists specializing in parallel databases and sharedcnothing architecturesc have been critical of the breadth of problems that mapreduce can be used forccccc they called its interface too lowclevel and questioned whether it really represents the paradigm shift its proponents have claimed it isccccc they challenged the mapreduce proponentsc claims of noveltyc citing teradata as an example of prior art that has existed for over two decadesc they also compared mapreduce programmers to codasyl programmersc noting both are cwriting in a lowclevel language performing lowclevel record manipulationcccccc mapreducecs use of input files and lack of schema support prevents the performance improvements enabled by common database system features such as bctrees and hash partitioningc though projects such as pig cor piglatincc sawzallc apache hiveccccc ysmartccccc hbasecccc and bigtablecccccccc are addressing some of these problemsc another articlec by greg jorgensenc rejects these viewsccccc jorgensen asserts that dewitt and stonebrakercs entire analysis is groundless as mapreduce was never designed nor intended to be used as a databasec dewitt and stonebraker have subsequently published a detailed benchmark study in cccc comparing performance of hadoopcs mapreduce and rdbms approaches on several specific problemsccccc they concluded that databases offer real advantages for many kinds of data usec especially on complex processing or where the data is used across an enterprisec but that mapreduce may be easier for users to adopt for simple or onectime processing tasksc they have published the data and code used in their study to allow other researchers to do comparable studiesc google has been granted a patent on mapreduceccccc howeverc there have been claims that this patent should not have been granted because mapreduce is too similar to existing productsc for examplec map and reduce functionality can be very easily implemented in oraclecs plcsql database oriented languageccccc apache hadoop is an opencsource software framework that supports datacintensive distributed applicationsc licensed under the apache vc licensecccc it enables applications to work with thousands of computationcindependent computers and petabytes of datac hadoop was derived from googlecs mapreduce and google file system cgfsc papersc the entire apache hadoop ccplatformcc is now commonly considered to consist of the hadoop kernelc mapreduce and hdfsc as well as a number of related projects cv including apache hivec apache hbasec and otherscccc hadoop is a topclevel apache project being built and used by a global community of contributorscccc written in the java programming languagec the apache hadoop project and its related projects chivec hbasec zookeeperc and so onc have many contributors from across the ecosystemcccc contents chidec c history c architecture ccc filesystems ccccc hadoop distributed file system ccccc other filesystems ccc jobtracker and tasktrackerc the mapreduce engine ccccc scheduling ccccccc fair scheduler ccccccc capacity scheduler ccc other applications c prominent users ccc yahooc ccc facebook ccc other users c hadoop on amazon ecccsc services ccc amazon elastic mapreduce c industry support of academic clusters c running hadoop in compute farm environments ccc grid engine integration ccc condor integration c commercially supported hadoopcrelated products ccc asfcs view on the use of chadoopc in product names c papers c see also cc references cc bibliography cc external links ceditchistory hadoop was created by doug cutting and michael jc cafarellacccc dougc who was working at yahoo at the timecccc named it after his soncs toy elephantcccc it was originally developed to support distribution for the nutch search engine projectcccc ceditcarchitecture hadoop consists of the hadoop common which provides access to the filesystems supported by hadoopc the hadoop common package contains the necessary jar files and scripts needed to start hadoopc the package also provides source codec documentationc and a contribution section which includes projects from the hadoop communityc for effective scheduling of workc every hadoopccompatible filesystem should provide location awarenessc the name of the rack cmore preciselyc of the network switchc where a worker node isc hadoop applications can use this information to run work on the node where the data isc andc failing thatc on the same rackcswitchc reducing backbone trafficc the hadoop distributed file system chdfsc uses this when replicating datac to try to keep different copies of the data on different racksc the goal is to reduce the impact of a rack power outage or switch failure so that even if these events occurc the data may still be readablecccc a multicnode hadoop cluster a small hadoop cluster will include a single master and multiple worker nodesc the master node consists of a jobtrackerc tasktrackerc namenodec and datanodec a slave or worker node acts as both a datanode and tasktrackerc though it is possible to have dataconly worker nodesc and computeconly worker nodesc these are normally only used in noncstandard applicationsc hadoop requires jre ccc or higherc the standard startup and shutdown scripts require ssh to be set up between nodes in the clusterc in a larger clusterc the hdfs is managed through a dedicated namenode server to host the filesystem indexc and a secondary namenode that can generate snapshots of the namenodecs memory structuresc thus preventing filesystem corruption and reducing loss of datac similarlyc a standalone jobtracker server can manage job schedulingc in clusters where the hadoop mapreduce engine is deployed against an alternate filesystemc the namenodec secondary namenode and datanode architecture of hdfs is replaced by the filesystemcspecific equivalentc ceditcfilesystems ceditchadoop distributed file system hdfs is a distributedc scalablec and portable filesystem written in java for the hadoop frameworkc each node in a hadoop instance typically has a single namenodec a cluster of datanodes form the hdfs clusterc the situation is typical because each node does not require a datanode to be presentc each datanode serves up blocks of data over the network using a block protocol specific to hdfsc the filesystem uses the tcpcip layer for communicationc clients use rpc to communicate between each otherc hdfs stores large files can ideal file size is a multiple of cc mbcccccc across multiple machinesc it achieves reliability by replicating the data across multiple hostsc and hence does not require raid storage on hostsc with the default replication valuec cc data is stored on three nodesc two on the same rackc and one on a different rackc data nodes can talk to each other to rebalance datac to move copies aroundc and to keep the replication of data highc hdfs is not fully posix compliant because the requirements for a posix filesystem differ from the target goals for a hadoop applicationc the tradeoff of not having a fully posix compliant filesystem is increased performance for data throughputc hdfs was designed to handle very large filesccccc hdfs has recently added highcavailability capabilitiesc allowing the main metadataserver cthe namenodec to be manually failed over to a backup in the event of failurec automatic failover is being developed as wellc additionallyc the filesystem includes what is called a secondary namenodec which misleads some people into thinking that when the primary namenode goes offlinec the secondary namenode takes overc in factc the secondary namenode regularly connects with the primary namenode and builds snapshots of the primary namenodecs directory informationc which is then saved to localcremote directoriesc these checkpointed images can be used to restart a failed primary namenode without having to replay the entire journal of filesystem actionsc then edit the log to create an upctocdate directory structurec since namenode is the single point for storage and management of metadatac this can be a bottleneck for supporting a huge number of filesc especially a large number of small filesc hdfs federation is a new addition which aims to tackle this problem to a certain extent by allowing multiple namespaces served by separate namenodes c an advantage of using hdfs is data awareness between the jobtracker and tasktrackerc the jobtracker schedules mapcreduce jobs to tasktrackers with an awareness of the data locationc an example of this would be if node a contained data cxcyczc and node b contained data cacbcccc the jobtracker will schedule node b to perform mapcreduce tasks on cacbccc and node a would be scheduled to perform mapcreduce tasks on cxcyczcc this reduces the amount of traffic that goes over the network and prevents unnecessary data transferc when hadoop is used with other filesystems this advantage is not always availablec this can have a significant impact on the performance of job completion timesc which has been demonstrated when running data intensive jobsccccc another limitation of hdfs is that it cannot be directly mounted by an existing operating systemc getting data into and out of the hdfs file systemc an action that often needs to be performed before and after executing a jobc can be inconvenientc a filesystem in userspace cfusec virtual file system has been developed to address this problemc at least for linux and some other unix systemsc file access can be achieved through the native java apic the thrift api to generate a client in the language of the usersc choosing ccccc javac pythonc phpc rubyc erlangc perlc haskellc ccc cocoac smalltalkc and ocamlcc the commandcline interfacec or browsed through the hdfscui webapp over httpc ceditcother filesystems by may ccccc the list of supported filesystems includedc hdfsc hadoopcs own rackcaware filesystemccccc this is designed to scale to tens of petabytes of storage and runs on top of the filesystems of the underlying operating systemsc amazon sc filesystemc this is targeted at clusters hosted on the amazon elastic compute cloud serverconcdemand infrastructurec there is no rackcawareness in this filesystemc as it is all remotec cloudstore cpreviously kosmos distributed file systemcc which is rackcawarec ftp filesystemc this stores all its data on remotely accessible ftp serversc readconly http and https file systemsc hadoop can work directly with any distributed file system that can be mounted by the underlying operating system simply by using a fileccc urlc howeverc this comes at a pricec the loss of localityc to reduce network trafficc hadoop needs to know which servers are closest to the datac this is information which hadoopcspecific filesystem bridges can providec outcofcthecboxc this includes amazon scc and the cloudstore filestorec through scccc and kfsccc urls directlyc a number of third party filesystem bridges have also been writtenc none of which are currently in hadoop distributionsc in cccc ibm discussed running hadoop over the ibm general parallel file systemccccc the source code was published in october ccccccccc in april ccccc parascale published the source code to run hadoop against the parascale filesystemccccc in april ccccc appistry released a hadoop filesystem driver for use with its own cloudiq storage productccccc in june ccccc hp discussed a locationcaware ibrix fusion filesystem driverccccc in may ccccc mapr technologiesc incc announced the availability of an alternate filesystem for hadoopc which replaced the hdfs file system with a full randomcaccess readcwrite file systemc with advanced features like snaphots and mirrorsc and get rid of the single point of failure issue of the default hdfs namenodec ceditcjobtracker and tasktrackerc the mapreduce engine above the file systems comes the mapreduce enginec which consists of one jobtrackerc to which client applications submit mapreduce jobsc the jobtracker pushes work out to available tasktracker nodes in the clusterc striving to keep the work as close to the data as possiblec with a rackcaware filesystemc the jobtracker knows which node contains the datac and which other machines are nearbyc if the work cannot be hosted on the actual node where the data residesc priority is given to nodes in the same rackc this reduces network traffic on the main backbone networkc if a tasktracker fails or times outc that part of the job is rescheduledc the tasktracker on each node spawns off a separate java virtual machine process to prevent the tasktracker itself from failing if the running job crashes the jvmc a heartbeat is sent from the tasktracker to the jobtracker every few minutes to check its statusc the job tracker and tasktracker status and information is exposed by jetty and can be viewed from a web browserc if the jobtracker failed on hadoop cccc or earlierc all ongoing work was lostc hadoop version cccc added some checkpointing to this processc the jobtracker records what it is up to in the filesystemc when a jobtracker starts upc it looks for any such datac so that it can restart work from where it left offc in earlier versions of hadoopc all active work was lost when a jobtracker restartedc known limitations of this approach arec the allocation of work to tasktrackers is very simplec every tasktracker has a number of available slots csuch as cc slotsccc every active map or reduce task takes up one slotc the job tracker allocates work to the tracker nearest to the data with an available slotc there is no consideration of the current system load of the allocated machinec and hence its actual availabilityc if one tasktracker is very slowc it can delay the entire mapreduce job c especially towards the end of a jobc where everything can end up waiting for the slowest taskc with speculativecexecution enabledc howeverc a single task can be executed on multiple slave nodesc ceditcscheduling by default hadoop uses fifoc and optional c scheduling priorities to schedule jobs from a work queueccccc in version cccc the job scheduler was refactored out of the jobtrackerc and added the ability to use an alternate scheduler csuch as the fair scheduler or the capacity schedulercccccc ceditcfair scheduler the fair scheduler was developed by facebookc the goal of the fair scheduler is to provide fast response times for small jobs and qos for production jobsc the fair scheduler has three basic conceptsccccc jobs are grouped into poolsc each pool is assigned a guaranteed minimum sharec excess capacity is split between jobsc by defaultc jobs that are uncategorized go into a default poolc pools have to specify the minimum number of map slotsc reduce slotsc and a limit on the number of running jobsc ceditccapacity scheduler the capacity scheduler was developed by yahooc the capacity scheduler supports several features which are similar to the fair schedulerccccc jobs are submitted into queuesc queues are allocated a fraction of the total resource capacityc free resources are allocated to queues beyond their total capacityc within a queue a job with a high level of priority will have access to the queuecs resourcesc there is no preemption once a job is runningc ceditcother applications the hdfs filesystem is not restricted to mapreduce jobsc it can be used for other applicationsc many of which are under development at apachec the list includes the hbase databasec the apache mahout machine learning systemc and the apache hive data warehouse systemc hadoop can in theory be used for any sort of work that is batchcoriented rather than realctimec that is very datacintensivec and able to work on pieces of the data in parallelc as of october ccccc commercial applications of hadoopcccc includedc log andcor clickstream analysis of various kinds marketing analytics machine learning andcor sophisticated data mining image processing processing of xml messages web crawling andcor text processing general archivingc including of relationalctabular datac ecgc for compliance ceditcprominent users ceditcyahooc on february ccc ccccc yahooc incc launched what it claimed was the worldcs largest hadoop production applicationc the yahooc search webmap is a hadoop application that runs on a more than cccccc core linux cluster and produces data that is now used in every yahooc web search queryccccc there are multiple hadoop clusters at yahoocc and no hdfs filesystems or mapreduce jobs are split across multiple datacentersc every hadoop cluster node bootstraps the linux imagec including the hadoop distributionc work that the clusters perform is known to include the index calculations for the yahooc search enginec on june ccc ccccc yahooc made available the source code to the version of hadoop it runs in productionccccc yahooc contributes back all work it does on hadoop to the opencsource communityc the companycs developers also fix bugs and provide stability improvements internallyc and release this patched source code so that other users may benefit from their effortc ceditcfacebook in cccc facebook claimed that they had the largest hadoop cluster In the world with cc pb of Storageccccc on july ccc cccc they announced the data had grown to cc pbccccc on june ccc cccc they announced the data had grown to ccc pbccccc 
